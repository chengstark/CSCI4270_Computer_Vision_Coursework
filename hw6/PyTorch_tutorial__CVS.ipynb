{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to PyTorch and Neural Networks\n",
    "#### Charles Stewart\n",
    "#### Last revision: November 18, 2019\n",
    "\n",
    "This is a brief tutorial introduction to PyTorch using a specific example of building\n",
    "a neural network that learns to classify points as inside or outside a hyper sphere\n",
    "of a given radius.\n",
    "\n",
    "We'll start with the problem itself, working purely in NumPy in order to clarify\n",
    "what we are trying to do.  After this, we'll introduce the basic notations of PyTorch, and proceed to two different neural network solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import numpy as np\n",
    "\n",
    "def gen_from_sphere(n, center, r_min, r_max):\n",
    "    '''\n",
    "    Generate n points in random directions from a center that are at least\n",
    "    r_min away and at most r_max away.  If r_min is 0 then the points are\n",
    "    within a hyper sphere of radius r_max.  Otherwise, the points are within a\n",
    "    spherical shell whose minimum radius is r_min and whose maximum radius\n",
    "    is r_max.  Note that the points generated in this simple way are not\n",
    "    uniformly distributed in Euclidean space, but that is not a major concern\n",
    "    here.\n",
    "    \n",
    "    The dimensionality of the space is inferred from the dimension of the center\n",
    "    point.  \n",
    "    \n",
    "    Return an array where each row is a data point. I.e. the array dimensions are (n, dim)\n",
    "    '''\n",
    "    dim = center.size\n",
    "    points = np.broadcast_to(center.reshape(1, dim), (n, dim)) \n",
    "    directions = 2 * np.random.random((n, dim)) - 1\n",
    "    directions /= np.linalg.norm(directions, axis=1).reshape((n, 1))\n",
    "    offsets = np.random.random(n) * (r_max - r_min) + r_min\n",
    "    points = points + directions * offsets.reshape((n, 1))\n",
    "    return points\n",
    "\n",
    "\n",
    "def gen_points_nested_spheres( n, center, r_min, r_max, debug=False):\n",
    "    '''\n",
    "    Generate n data points in hypersphere, half of which - the \"inliers\" --\n",
    "    are within r_min of the center and half of which -- the \"outliers\" -- are\n",
    "    between r_min and r_max.  Returned are the points in an n X d array\n",
    "    (dim is the dimension of the center point) and a n X 2 binary array\n",
    "    labeling whether the points are inliers (1,0) or outliers (0,1).\n",
    "    \n",
    "    If the debug flag is on, the array shape of the points and the labels\n",
    "    are output.  In addition, if the dimension is 2 then a plot is generated.\n",
    "    '''\n",
    "    num_inliers = n//2\n",
    "    num_outliers = n - num_inliers\n",
    "    inlier_X = gen_from_sphere(num_inliers, center, 0, r_min)\n",
    "    inlier_Y = np.broadcast_to(np.array([1., 0.]), (num_inliers, 2))\n",
    "    outlier_X = gen_from_sphere(num_outliers, center, r_min, r_max)\n",
    "    outlier_Y = np.broadcast_to(np.array([0., 1.]), (num_outliers, 2))\n",
    "    X = np.concatenate((inlier_X, outlier_X), axis=0)\n",
    "    Y = np.concatenate((inlier_Y, outlier_Y), axis=0)\n",
    "    \n",
    "    if debug:\n",
    "        print('Generated points in array of dimension', X.shape)\n",
    "        print('Binary labels are in a second array of dimension', Y.shape)\n",
    "        \n",
    "        if center.size == 2:\n",
    "            from matplotlib import pyplot as plt\n",
    "            plt.scatter(inlier_X[:,0], inlier_X[:,1], marker='+' )\n",
    "            plt.scatter(outlier_X[:,0], outlier_X[:,1], marker='o' )\n",
    "            plt.axis('equal')\n",
    "            plt.show()\n",
    "\n",
    "    return X,Y\n",
    "\n",
    "# Generate an example in 2d, just to see the point distribution\n",
    "# The \"real\" data will be in higher dimensions. \n",
    "num_points = 400\n",
    "center = np.array([1.5, 6.4])\n",
    "r_min, r_max = 5, 10\n",
    "X,Y = gen_points_nested_spheres(num_points, center, r_min, r_max, debug=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch\n",
    "\n",
    "Now for PyTorch.  The starting point is http://pytorch.org/tutorials/ for tutorials and http://pytorch.org/docs/master/ for extensive documentation.  On the tutorials site, I found both the \"60 Minute Blitz\" and \"Learning PyTorch with Examples\" to be useful, but preferred the latter.  It starts with a simple neural network implemented in NumPy, then converts this to use of PyTorch Tensors.  Next it introduces Variables and their most important property, automatic gradient calculation (\"autograd\").  At that point it has two subsections, one on defining new autograd functions and the second comparing to TensorFlow Static Graphs.  Neither of these is terribly important on first reading.  Instead, I'd suggest skipping ahead to the nn module.  I suggest going back to the 60 Minute Blitz for examples of convolutional layers.\n",
    "\n",
    "We'll start here with a brief discussion of Tensors and Variables and then jump right into Neural Networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# The first important property of torch is the notion of a Tensor.  This is essentially\n",
    "# the same as a NumPy array, but it has CUDA / GPU capabilities when a GPU is available.\n",
    "# We can easily convert back and forth between Tensors and arrays without deep copying\n",
    "\n",
    "# The following creates a 2d tensor with 6 rows and 3 columns, filled in with random\n",
    "# values from a normal distribution of mean 0 and variance 1.\n",
    "w = torch.randn(6, 3)\n",
    "print('First tensor: w')\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Like NumPy arrays, tensors can be sliced and this occurs without deep copying\n",
    "x = w[2:4, 0:2]\n",
    "print('Sliced verson of W is x:')\n",
    "print(x)\n",
    "x[0,1] = 20\n",
    "print('We changed x[0,1] to', x[0,1], 'and now w[2,1] is also', w[2,1])\n",
    "print(\"w is now\\n\", w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Create a row vector and then do component-wise multiplication\n",
    "u = torch.randn(3)\n",
    "print('u', u)\n",
    "print('3-component vector as a tensor:', u)\n",
    "print('Component-wise multiplication, with broadcasting')\n",
    "print(w*u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Now create a 3x1 tensor--- essentially a column vector --- and do\n",
    "#  matrix multiplication\n",
    "u = torch.randn(3, 1)\n",
    "print('u', u)\n",
    "print(torch.mm(w,u))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Many of the methods you'd expect for NumPy arrays exist for PyTorch tensors.  See\n",
    "#  extensive documentation at http://pytorch.org/docs/master/torch.html#tensors\n",
    "\n",
    "# We can go back and forth between NumPy arrays and PyTorch tensors\n",
    "# through shallow copies\n",
    "a = u.numpy()\n",
    "print('a is the numpy version of u:\\n', a)\n",
    "b = torch.from_numpy(a)\n",
    "c = torch.Tensor(b)\n",
    "print('b and c are back to pytorch:', b)\n",
    "a[0,0] = 5\n",
    "print('We changed a[0,0] and this changes b and c as well:')\n",
    "print('b[0,0] =', b[0,0], 'and c[0,0] =', c[0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So why create a separate class for tensors when they seem to have\n",
    "# the same attributes as as NumPy arrays?  There are two answers.\n",
    "\n",
    "# First, tensors are designed to work with CUDA when a GPU is available,\n",
    "# but this is transparent to the rest of the code.  Here is one way to \n",
    "# handle this, which is enough for now, but there are others in the PyTorch\n",
    "# library.\n",
    "if torch.cuda.is_available():\n",
    "    w = w.cuda()\n",
    "    b = b.cuda()\n",
    "\n",
    "c = torch.mm(w,b)\n",
    "print('Tensor c is\\n', c)  # Proceeds whether on CPU or GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch Variables\n",
    "The second reason for PyTorch tensors is that they are designed to work with our second\n",
    "major PyTorch construct:  Variables.  A Variable has three major attributes:\n",
    "1. data --- A PyTorch tensor.   \n",
    "2. grad --- The gradient tensor of the variable with respect to the final cost function it is used in\n",
    "3. creator - The link to the function the Variable is created from.  More on this below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Reminding ourselves:  w=\\n', w)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now create a variable from w, telling the constructor that we want\n",
    "# to be able to compute gradients on this variable.\n",
    "wv = Variable(w, requires_grad=True)\n",
    "print(\"wv=\\n\", wv)\n",
    "print('wv.data =\\n', wv.data)\n",
    "print('Slicing works on variables:\\n', wv[2:4,1:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Next create a simple cost function\n",
    "cost = torch.mean(wv * wv)  # Cost is the square magnitude of the elements of the matrix\n",
    "print('cost is', cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now comes the gradient!\n",
    "By calling the method *backward*, we backpropagate the gradient through the computation used to create the cost function value.  In this simple case, we are computing the gradient with respect to the terms of our our small $m \\times n$ ($ 6 \\times 3$) 2d tensor.  The result should be another 2d tensor whose values are $\\frac{2}{m n} w[i,j]$. When we ask for the values of the gradient we get exactly what we predicted.  For example, the 2,2 entry in the gradient, corresponding to the value 20 in wv, is $2/18\\times 20$ as the following shows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost.backward()\n",
    "print('wv.grad=\\n', wv.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One note about the calculation of the gradient through the call to the backward method is that by default PyTorch releases intermediate buffers for the gradient computation at the end of the backward calculation through the graph.  If you run the previous step a second time - without going further up and redefining the cost function - you will get a run-time error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Creator / grad_fn\n",
    "\n",
    "In order to briefly discuss the creator function, we take this further in the following example by combining several variables, similar to the computation in a fully connected layer of a neural network.  This combination can be thought of as a simple graph where the root is the cost function, each node in the middle is the result of an intermediate function, and our leaves are the initial variables we are interested in. The nodes have grad_fn that point back to the previous nodes (or leaves) combined to create them.  Leaves have no grad_fn.\n",
    "\n",
    "The print statements from the following only show these grad_fn values, but if you wish you can add print statements to calculate the gradients.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "av = Variable(torch.randn(3, 1), requires_grad=True)\n",
    "bv = Variable(torch.randn(6, 1), requires_grad=True)\n",
    "mult = torch.mm(wv, av) + bv\n",
    "cost = torch.mean(mult)\n",
    "cost.backward()\n",
    "print('cost.grad_fn:', cost.grad_fn)\n",
    "print('mult.grad_fn:', mult.grad_fn)\n",
    "print('av.grad_fn:', av.grad_fn)\n",
    "print('bv.grad_fn:', bv.grad_fn)\n",
    "print('wv.grad_fn:', wv.grad_fn)\n",
    "print('av.grad:\\n', av.grad)\n",
    "print('bv.grad:\\n', bv.grad)\n",
    "print('mult.grad:', mult.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Networks in PyTorch\n",
    "Using Variables and the build up of functions between them, we can create neural network by hand.  Doing so is a good exercise in understanding the basics of PyTorch and if you are going to do significant work in neural networks I recommend that you do so.  For our purposes, however, we are going to switch gears to the higher-level capabilities in PyTorch for defining an using neural networks from standard components.\n",
    "\n",
    "Before getting started, we return to our hypersphere classification problem, and generate the data we will actually use to train, validate and test our neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  The points, the center location and the dimensions of the \n",
    "#  internal hypersphere and the external disk.\n",
    "point_dim = 4\n",
    "center = 10*np.random.randn(point_dim)\n",
    "r_min, r_max = 4, 8\n",
    "\n",
    "def generate_and_convert(n, center, r_min, r_max, debug=True, name=''):\n",
    "    '''\n",
    "    Generate the data for the interior and exterior points and convert\n",
    "    to PyTorch variables.\n",
    "    '''\n",
    "    X, Y = gen_points_nested_spheres( n, center, r_min, r_max, debug)\n",
    "    X, Y = Variable(torch.Tensor(X)), Variable(torch.Tensor(Y))\n",
    "    if debug:\n",
    "        print('generate_and_convert for', name, ' X size', X.size(), ' Y size', Y.size())\n",
    "    return X, Y\n",
    "\n",
    "n_train, n_valid, n_test = 1000, 128, 128\n",
    "X_train, Y_train = generate_and_convert(n_train, center, r_min, r_max, debug=True, name='Train')\n",
    "X_valid, Y_valid = generate_and_convert(n_valid, center, r_min, r_max, debug=True, name='Valid')\n",
    "X_test, Y_test = generate_and_convert(n_test, center, r_min, r_max, debug=True, name='Valid')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining a simple fully connected network\n",
    "\n",
    "The cell below shows the definition of a neural network with two hidden layers of 15 nodes each.\n",
    "It is defined as a subclass of the nn.Module class, so the first step in the initializer is to call the initializer for the base class.\n",
    "The initializer for our new class creates the layers --- each fully-connected --- but does not hook them together.  This is done in the forward function.\n",
    "You will also notice that there is no definition of the backward function, even though it is used.\n",
    "This function is created in the base class based on the result of the forward function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "N0 = point_dim\n",
    "N1 = 15\n",
    "N2 = 15\n",
    "Nout = 2  \n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        # Create three fully connected layers, two of which are hidden and the third is\n",
    "        # the output layer.  In each case, the first argument to Linear is the number\n",
    "        # of input values from the previous layer, and the second argument is the number\n",
    "        # of nodes in this layer.  The call to the Linear initializer creates a PyTorch\n",
    "        # functional that in turn adds a weight matrix and a bias vector to the list of\n",
    "        # (learnable) parameters stored with each Net object.  These weight matrices\n",
    "        # and bias vectors are implicitly initialized using a uniform random distribution,\n",
    "        # in the range [-1/sqrt(k), 1/sqrt(k)] where k is the number of units at the\n",
    "        # previous layer.\n",
    "        self.fc1 = nn.Linear(N0, N1, bias=True)\n",
    "        self.fc2 = nn.Linear(N1, N2, bias=True)\n",
    "        self.fc3 = nn.Linear(N2, Nout, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #  The forward method takes an input Variable and creates a chain of Variables\n",
    "        #  from the layers of the network defined in the initializer. The F.relu is\n",
    "        #  a functional implementing the Rectified Linear activation function.\n",
    "        #  Notice that the output layer does not include the activation function.\n",
    "        #  As we will see, that is combined into the criterion for the loss function.\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "#  Create an instance of this network.\n",
    "net = Net()\n",
    "\n",
    "#  Define the Mean Squared error loss function as the criterion for this network's training\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "#  Print a summary of the network.  Notice that this only shows the layers\n",
    "print(net)\n",
    "\n",
    "#  Print a list of the parameter sizes\n",
    "params = list(net.parameters())\n",
    "print(params[0].size()) # The parameter holding the layer 1 weight matrix\n",
    "print(params[1].size()) # ... the layer 1 bias vector\n",
    "print(params[2].size()) # ... the layer 2 weight matrix\n",
    "print(params[3].size()) # ... the layer 2 bias vector\n",
    "print(params[4].size()) # ... the layer 3 weight vector\n",
    "print(params[5].size()) # ... the layer 3 bias vector\n",
    "\n",
    "print(params[0])\n",
    "print(params[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Training the network\n",
    "\n",
    "Now we can write our own training function using a form of stochastic gradient descent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Set parameters to control the process\n",
    "epochs = 1000\n",
    "batch_size = 16\n",
    "n_batches = int(np.ceil(n_train / batch_size))\n",
    "learning_rate = 1e-4\n",
    "\n",
    "#  Compute an initial loss using all of the validation data.\n",
    "#\n",
    "#  A couple of notes are important here:\n",
    "#  (1) X_valid contains all of the validation input, with each validation\n",
    "#      data instance being a row of X_valid\n",
    "#  (2) Therefore, pred_Y_valid is a Variable containing the output layer\n",
    "#      activations for each of the validation inputs.\n",
    "#  (3) This is accomplished through the function call net(X_valid), which in\n",
    "#      turn calls the forward method under the hood to figure out the flow of\n",
    "#      the data and activations in the network.\n",
    "pred_Y_valid = net(X_valid)\n",
    "valid_loss = criterion(pred_Y_valid, Y_valid)\n",
    "print(\"Initial loss: %.5f\" % valid_loss.item())\n",
    "\n",
    "for ep in range(epochs):\n",
    "    #  Create a random permutation of the indices of the row vectors.\n",
    "    indices = torch.randperm(n_train)\n",
    "    \n",
    "    #  Run through each mini-batch\n",
    "    for b in range(n_batches):\n",
    "        #  Use slicing (of the pytorch Variable) to extract the\n",
    "        #  indices and then the data instances for the next mini-batch\n",
    "        batch_indices = indices[b*batch_size: (b+1)*batch_size]\n",
    "        batch_X = X_train[batch_indices]\n",
    "        batch_Y = Y_train[batch_indices]\n",
    "        \n",
    "        #  Run the network on each data instance in the minibatch\n",
    "        #  and then compute the object function value\n",
    "        pred_Y = net(batch_X)\n",
    "        loss = criterion(pred_Y, batch_Y)\n",
    "        \n",
    "        #  Back-propagate the gradient through the network using the\n",
    "        #  implicitly defined backward function, but zero out the\n",
    "        #  gradient first.\n",
    "        net.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        #  Complete the mini-batch by actually updating the parameters.\n",
    "        for param in net.parameters():\n",
    "            param.data -= learning_rate * param.grad.data\n",
    "            \n",
    "    #  Print validation loss every 10 epochs\n",
    "    if ep != 0 and ep%10 == 0:\n",
    "        pred_Y_valid = net(X_valid)\n",
    "        valid_loss = criterion(pred_Y_valid, Y_valid)\n",
    "        print(\"Epoch %d loss: %.5f\" %(ep, valid_loss.item()))\n",
    "\n",
    "#  Compute and print the final training and test loss\n",
    "#  function values\n",
    "pred_Y_train = net(X_train)\n",
    "loss = criterion(pred_Y_train, Y_train)\n",
    "print('Final training loss is %.5f' %loss.item())\n",
    "\n",
    "pred_Y_test = net(X_test)\n",
    "test_loss = criterion(pred_Y_test, Y_test)\n",
    "print(\"Final test loss: %.5f\" % test_loss.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Success Rate\n",
    "\n",
    "The loss function value does not indicate the actual success rate.  In fact the MSE loss can in theory be arbitrarily bad and we could still have perfect classification.  By increasing the size of the training data and the number of epochs we can obtain better and better classification rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def success_rate(pred_Y, Y):\n",
    "    '''\n",
    "    Calculate and return the success rate from the predicted output Y and the\n",
    "    expected output.  There are several issues to deal with.  First, the pred_Y\n",
    "    is non-binary, so the classification decision requires finding which column\n",
    "    index in each row of the prediction has the maximum value.  This is achieved\n",
    "    by using the torch.max() method, which returns both the maximum value and the\n",
    "    index of the maximum value; we want the latter.  We do this along the column,\n",
    "    which we indicate with the parameter 1.  Second, the once we have a 1-d vector\n",
    "    giving the index of the maximum for each of the predicted and target, we just\n",
    "    need to compare and count to get the number that are different.  We could do\n",
    "    using the Variable objects themselves, but it is easier syntactically to do this\n",
    "    using the .data Tensors for obscure PyTorch reasons.\n",
    "    '''\n",
    "    _,pred_Y_index = torch.max(pred_Y, 1)\n",
    "    _,Y_index = torch.max(Y,1)\n",
    "    num_equal = torch.sum(pred_Y_index.data == Y_index.data).item()\n",
    "    num_different = torch.sum(pred_Y_index.data != Y_index.data).item()\n",
    "    rate = num_equal / float(num_equal + num_different)\n",
    "    return rate \n",
    "\n",
    "print('Training success rate:', success_rate(pred_Y_train, Y_train))\n",
    "print('Test success rate:', success_rate(pred_Y_test, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Cross Entropy, SoftMax and SGD\n",
    "\n",
    "As a final step in our initial learning of PyTorch, we'll examine use of the Cross-entropy loss function along with the softmax.  Then we'll examine use of built-in optimizers, in particular stochastic gradient descent (SGD).  Normally we think of SGD as having mini-batches and permutations associated with it, but that must be done explicitly here.  PyTorch does provide tools to help us, but this tutorial does not dig into them.\n",
    "\n",
    "Unfortunately, SGD requires the target outputs to be categorical indices as rather than binary vectors. For example, the instead of [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.] to represent the digit 3, we just use the value 3. In addition, these must be tensors of longs rather than tensors of doubles.   We could go back and change the code at the very beginning to make both of these corrections, but instead we use properties of PyTorch to make the changes here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_categories(Y):\n",
    "    _, categories = torch.max(Y.data, 1)\n",
    "    categories = torch.Tensor.long(categories)\n",
    "    return Variable(categories)\n",
    "\n",
    "Y_test_c = convert_to_categories(Y_test)\n",
    "Y_train_c = convert_to_categories(Y_train)\n",
    "Y_valid_c = convert_to_categories(Y_valid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=1e-4)\n",
    "\n",
    "pred_Y_valid = net(X_valid)\n",
    "valid_loss = criterion(pred_Y_valid, Y_valid_c)\n",
    "print('Initial validation loss: %.5f' % valid_loss.item())\n",
    "\n",
    "num_epochs = 3000\n",
    "for ep in range(num_epochs):\n",
    "    #  Create a random permutation of the indices of the row vectors.\n",
    "    indices = torch.randperm(n_train)\n",
    "    \n",
    "    #  Run through each mini-batch\n",
    "    for b in range(n_batches):\n",
    "        #  Use slicing (of the pytorch Variable) to extract the\n",
    "        #  indices and then the data instances for the next mini-batch\n",
    "        batch_indices = indices[b*batch_size:(b+1)*batch_size]\n",
    "        batch_X = X_train[batch_indices]\n",
    "        batch_Y = Y_train_c[batch_indices]\n",
    "        \n",
    "        #  Run the network on each data instance in the minibatch\n",
    "        #  and then compute the object function value\n",
    "        pred_Y = net(batch_X)\n",
    "        loss = criterion(pred_Y, batch_Y)\n",
    "        \n",
    "        #  Back-propagate the gradient through the network using the\n",
    "        #  implicitly defined backward function, but zero out the\n",
    "        #  gradient first.  The step is made here by the optimizer\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    pred_Y = net(X_train)\n",
    "    loss = criterion(pred_Y, Y_train_c)\n",
    "    if ep !=0 and ep%10==0:\n",
    "        pred_Y_valid = net(X_valid)\n",
    "        valid_loss = criterion(pred_Y_valid, Y_valid_c)\n",
    "        print(\"Epoch %d loss: %.5f\" %(ep, valid_loss.item()))\n",
    "\n",
    "pred_Y_train = net(X_train)\n",
    "pred_Y_test = net(X_test)\n",
    "print('Training success rate:', success_rate(pred_Y_train, Y_train))\n",
    "print('Test success rate:', success_rate(pred_Y_test, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "These results are by no means perfect and can be easily improved.  However, our emphasis has been on the basics.  I strongly recommend studying these examples and playing with similar code on your own.  Then, just jump into the HW 6 assignment.  You really can write the solutions with relatively little code.  You are also welcome to use as much of the code I've provided here as you wish.  The only thing I have not covered that you need is the implementation of convolutional networks, but you can find examples that will help you get started there."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}